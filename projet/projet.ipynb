{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_terms(text):\n",
    "    # Utilise une expression régulière pour rechercher le terme préféré\n",
    "    pref_t = None\n",
    "    n_pref_term = []\n",
    "\n",
    "    # Utilise une expression régulière pour rechercher le terme préféré\n",
    "    match = re.search(r\"^1: (.+)\", text, re.MULTILINE)\n",
    "    if match:\n",
    "        pref_t = match.group(1).strip()\n",
    "        # Si le terme préféré est suivi de \"[Supplementary Concept]\"\n",
    "        if \"[Supplementary Concept]\" in pref_t:\n",
    "            # Extrait le terme sans le suffixe\n",
    "            pref_t = pref_t.split(\"[Supplementary Concept]\")[0].strip()\n",
    "\n",
    "    # Utilise des expressions régulières pour trouver les entrées des termes préférés\n",
    "    entry_terms_match = re.search(r\"Entry Terms:\\s*(.*?)\\n\\n\", text, re.DOTALL)\n",
    "    if entry_terms_match:\n",
    "        entry_terms = entry_terms_match.group(1).strip().split(\"\\n\")\n",
    "        # Les termes préférés se trouvent après les entrées de termes\n",
    "        n_pref_term = [term.strip() for term in entry_terms if term.strip()]\n",
    "\n",
    "    return pref_t, n_pref_term\n",
    "\n",
    "\n",
    "def search_mesh(query):\n",
    "    # Paramètres de la requête\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "    search_url = base_url + \"esearch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"mesh\",  # Base de données MeSH\n",
    "        \"term\": query,  # Terme de recherche\n",
    "        \"retmode\": \"json\",  # Format de sortie JSON\n",
    "    }\n",
    "\n",
    "    # Effectuer la requête HTTP\n",
    "    response = requests.get(search_url, params=params)\n",
    "\n",
    "    # Vérifier le code de statut de la réponse\n",
    "    if response.status_code == 200:\n",
    "        mesh_ids = []\n",
    "        # Extraire les identifiants des concepts MeSH pertinents depuis la réponse JSON\n",
    "        try:\n",
    "            mesh_ids = response.json()[\"esearchresult\"][\"idlist\"]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                mesh_ids = response.json()[\"esearchresult\"][\"IdList\"]\n",
    "            except KeyError:\n",
    "                mesh_ids = []\n",
    "\n",
    "        return mesh_ids\n",
    "    else:\n",
    "        print(\"Erreur lors de la requête HTTP:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "\n",
    "# Fonction pour récupérer les informations détaillées sur un concept MeSH\n",
    "def get_mesh_details(mesh_id):\n",
    "    # Paramètres de la requête\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "    detail_url = base_url + \"efetch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"mesh\",  # Base de données MeSH\n",
    "        \"id\": mesh_id,  # Identifiant du concept MeSH\n",
    "        \"retmode\": \"xml\",  # Format de sortie JSON\n",
    "    }\n",
    "\n",
    "    # Effectuer la requête HTTP\n",
    "    response = requests.get(detail_url, params=params)\n",
    "\n",
    "    # Vérifier le code de statut de la réponse\n",
    "    if response.status_code == 200:\n",
    "        return response\n",
    "    else:\n",
    "        print(\"Erreur lors de la requête HTTP:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "\n",
    "def mesh_concept(term):\n",
    "    mesh_ids = search_mesh(term)\n",
    "    if len(mesh_ids) == 0:\n",
    "        return \"None\"\n",
    "    id = mesh_ids[0]\n",
    "    mesh_details = get_mesh_details(id)\n",
    "    if mesh_details:\n",
    "        p, np = extract_terms(mesh_details.text)\n",
    "        return [p, *np], id\n",
    "    else:\n",
    "        return \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Quantum Dots', 'Dot, Quantum', 'Dots, Quantum', 'Quantum Dot', 'Semiconductor Nanoparticles', 'Nanoparticle, Semiconductor', 'Nanoparticles, Semiconductor', 'Semiconductor Nanoparticle', 'Semiconductor Nanocrystals', 'Nanocrystal, Semiconductor', 'Semiconductor Nanocrystal', 'Nanocrystals, Semiconductor']\n"
     ]
    }
   ],
   "source": [
    "concept, _id = mesh_concept(\"confinement\")\n",
    "print(concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` markdown\n",
    "**Pretraitement**\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "def preprocess_text(\n",
    "    text,\n",
    "    lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_digits=True,\n",
    "    remove_stopwords=True,\n",
    "    lemma=False,\n",
    "    stem=False,\n",
    "):\n",
    "\n",
    "    # Tokenize the sentence into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    processed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        # Supprimer la ponctuation\n",
    "        if remove_punctuation:\n",
    "            p = string.punctuation\n",
    "            p += \"\\n\\r\\t\"  # Ajouter les retours chariot, tabulation\n",
    "            word = word.translate(str.maketrans(p, \" \" * len(p)))\n",
    "            clean_word = re.sub(r\"\\s[a-z]\\s\", \" \", word)\n",
    "            word = clean_word.strip()\n",
    "            word = (\n",
    "                unicodedata.normalize(\"NFD\", word)\n",
    "                .encode(\"ascii\", \"ignore\")\n",
    "                .decode(\"utf-8\")\n",
    "            )\n",
    "\n",
    "        # Convertir en minuscules\n",
    "        if lowercase:\n",
    "            word = word.lower()\n",
    "\n",
    "        # Supprimer les chiffres\n",
    "        if remove_digits:\n",
    "            word = re.sub(\n",
    "                \"[0-9]+\", \"\", word\n",
    "            )  # Remplacer une séquence de chiffres par rien\n",
    "\n",
    "        # Supprimer les stopwords\n",
    "        if remove_stopwords:\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "\n",
    "        # Lemmatisation\n",
    "        if lemma:\n",
    "            lem = WordNetLemmatizer()\n",
    "            word = lem.lemmatize(word)\n",
    "\n",
    "        if stem:\n",
    "            ps = PorterStemmer()\n",
    "            word = ps.stem(word)\n",
    "\n",
    "        processed_words.append(word)\n",
    "\n",
    "    # # Join the processed words back into a sentence\n",
    "    processed_text = \" \".join(processed_words).strip()\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "text = \"[(]The quick-brown-5fox jumps over the lazy dog! 123\"\n",
    "processed_text = preprocess_text(text, lemma=True)\n",
    "print(processed_text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk-22\"\n",
    "if not pt.started():\n",
    "    pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.datasets.get_dataset(\"irds:cord19/trec-covid\")\n",
    "indexer = pt.index.IterDictIndexer(\n",
    "    \"C:\\\\Users\\\\Saliou\\\\OneDrive\\\\Documents\\\\ALL TP\\\\RI\\\\projet\\\\cord19-index\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cord19/trec-covid documents:   0%|          | 0/192509 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:19:27.275 [main] WARN org.terrier.structures.FSADocumentIndex - This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cord19/trec-covid documents:   1%|          | 2149/192509 [00:03<04:02, 784.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:19:30.778 [main] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (8is9x9sc) - further warnings are suppressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cord19/trec-covid documents: 100%|██████████| 192509/192509 [01:31<00:00, 2104.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:20:58.663 [main] ERROR org.terrier.structures.indexing.Indexer - Could not finish MetaIndexBuilder: \n",
      "java.io.IOException: Key 8lqzfj2e is not unique: 37597,11755\n",
      "For MetaIndex, to suppress, set metaindex.compressed.reverse.allow.duplicates=true\n",
      "\tat org.terrier.structures.collections.FSOrderedMapFile$MultiFSOMapWriter.mergeTwo(FSOrderedMapFile.java:1374)\n",
      "\tat org.terrier.structures.collections.FSOrderedMapFile$MultiFSOMapWriter.close(FSOrderedMapFile.java:1308)\n",
      "\tat org.terrier.structures.indexing.BaseMetaIndexBuilder.close(BaseMetaIndexBuilder.java:321)\n",
      "\tat org.terrier.structures.indexing.classical.BasicIndexer.indexDocuments(BasicIndexer.java:270)\n",
      "\tat org.terrier.structures.indexing.classical.BasicIndexer.createDirectIndex(BasicIndexer.java:388)\n",
      "\tat org.terrier.structures.indexing.Indexer.index(Indexer.java:377)\n",
      "18:21:05.000 [main] WARN org.terrier.structures.indexing.Indexer - Indexed 60 empty documents\n",
      "18:21:05.008 [main] ERROR org.terrier.structures.indexing.Indexer - Could not rename index\n",
      "java.io.IOException: Rename of index structure file 'C:\\Users\\Saliou\\OneDrive\\Documents\\ALL TP\\RI\\projet\\cord19-index/data_1.direct.bf' (exists) to 'C:\\Users\\Saliou\\OneDrive\\Documents\\ALL TP\\RI\\projet\\cord19-index/data.direct.bf' (exists) failed - likely that source file is still open. Possible indexing bug?\n",
      "\tat org.terrier.structures.IndexUtil.renameIndex(IndexUtil.java:379)\n",
      "\tat org.terrier.structures.indexing.Indexer.index(Indexer.java:388)\n",
      "18:21:05.043 [main] WARN org.terrier.structures.FSADocumentIndex - This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n"
     ]
    }
   ],
   "source": [
    "indexref = indexer.index(dataset.get_corpus_iter(), fields=(\"title\", \"abstract\"))\n",
    "index = pt.IndexFactory.of(indexref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 192509\n",
      "Number of terms: 158515\n",
      "Number of postings: 12290426\n",
      "Number of fields: 2\n",
      "Number of tokens: 19603234\n",
      "Field names: [title, abstract]\n",
      "Positions:   false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# statistiques de la collection\n",
    "print(index.getCollectionStatistics().toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>78615</td>\n",
       "      <td>2gvvwvvg</td>\n",
       "      <td>0</td>\n",
       "      <td>3.691090</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>73872</td>\n",
       "      <td>azir1gvm</td>\n",
       "      <td>1</td>\n",
       "      <td>3.661167</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13737</td>\n",
       "      <td>t7sn9ffh</td>\n",
       "      <td>2</td>\n",
       "      <td>3.645867</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>185382</td>\n",
       "      <td>xykob69g</td>\n",
       "      <td>3</td>\n",
       "      <td>3.639548</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>82198</td>\n",
       "      <td>7972ps41</td>\n",
       "      <td>4</td>\n",
       "      <td>3.628132</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1</td>\n",
       "      <td>75601</td>\n",
       "      <td>5w39q98c</td>\n",
       "      <td>995</td>\n",
       "      <td>3.224067</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1</td>\n",
       "      <td>81443</td>\n",
       "      <td>knr07yy1</td>\n",
       "      <td>996</td>\n",
       "      <td>3.224067</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "      <td>82053</td>\n",
       "      <td>bhfgu5wg</td>\n",
       "      <td>997</td>\n",
       "      <td>3.224067</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1</td>\n",
       "      <td>87342</td>\n",
       "      <td>qrg6nrrm</td>\n",
       "      <td>998</td>\n",
       "      <td>3.224067</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "      <td>136812</td>\n",
       "      <td>yvo6qvuf</td>\n",
       "      <td>999</td>\n",
       "      <td>3.224067</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid   docid     docno  rank     score        query\n",
       "0     1   78615  2gvvwvvg     0  3.691090  coronavirus\n",
       "1     1   73872  azir1gvm     1  3.661167  coronavirus\n",
       "2     1   13737  t7sn9ffh     2  3.645867  coronavirus\n",
       "3     1  185382  xykob69g     3  3.639548  coronavirus\n",
       "4     1   82198  7972ps41     4  3.628132  coronavirus\n",
       "..   ..     ...       ...   ...       ...          ...\n",
       "995   1   75601  5w39q98c   995  3.224067  coronavirus\n",
       "996   1   81443  knr07yy1   996  3.224067  coronavirus\n",
       "997   1   82053  bhfgu5wg   997  3.224067  coronavirus\n",
       "998   1   87342  qrg6nrrm   998  3.224067  coronavirus\n",
       "999   1  136812  yvo6qvuf   999  3.224067  coronavirus\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = bm25.search(\"coronavirus\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarite(bm25_doc, index_concept, docno_doc, docno_conc, concept):\n",
    "    cj = [0]  # poids des termes du concept\n",
    "    dj = [0]  # poids des termes du document\n",
    "\n",
    "    bm25 = pt.BatchRetrieve(index_concept, wmodel=\"BM25\")\n",
    "\n",
    "    # parcourir chaque term du concept et verifier s'il est dans le document\n",
    "    concept = concept.split(\" \")\n",
    "    for term in concept:\n",
    "        # bm25 pour les documents\n",
    "        res1 = bm25_doc.search(term)  # chercher le terme dans le document\n",
    "        res1 = res1[\"score\"][\n",
    "            res1[\"docno\"] == docno_doc\n",
    "        ]  # chercher le score du terme dans le document\n",
    "\n",
    "        if len(res1) == 0:\n",
    "            continue\n",
    "\n",
    "        # bm25 pour les concepts\n",
    "        res2 = bm25.search(term)  # chercher le terme dans le concept\n",
    "        res2 = res2[\"score\"][res2[\"docno\"] == docno_conc]\n",
    "\n",
    "        if len(res2) == 0:\n",
    "            continue\n",
    "\n",
    "        dj.append(res1)\n",
    "        cj.append(res2)\n",
    "\n",
    "    # calculer la similarité\n",
    "    numerateur = sum([c * d for c, d in zip(cj, dj)])\n",
    "    denominateur = np.sqrt(sum([c**2 for c in cj])) * np.sqrt(sum([d**2 for d in dj]))\n",
    "\n",
    "    if denominateur == 0:\n",
    "        return 0\n",
    "\n",
    "    return numerateur / denominateur\n",
    "\n",
    "\n",
    "def average_position(occurrences):\n",
    "    return sum(occurrences) / len(occurrences)\n",
    "\n",
    "\n",
    "def word_positions(words):\n",
    "    positions = {}\n",
    "    for index, word in enumerate(words):\n",
    "        if word in positions:\n",
    "            positions[word].append(index)\n",
    "        else:\n",
    "            positions[word] = [index]\n",
    "    avg_positions = {word: average_position(pos) for word, pos in positions.items()}\n",
    "    avg_positions = sorted(avg_positions.items(), key=lambda x: x[1])\n",
    "    list_words = [word for word, _ in avg_positions]\n",
    "    return list_words\n",
    "\n",
    "\n",
    "def normalize_document(words, concept_words):\n",
    "\n",
    "    start, end = -1, -1\n",
    "    for i, word in enumerate(words):\n",
    "        if word in concept_words:\n",
    "            if start == -1:\n",
    "                start = i\n",
    "            end = i\n",
    "    if start == -1 or end == -1:\n",
    "        return []\n",
    "    return words[start : end + 1]\n",
    "\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "def spearman_rank_correlation(normalize_document, concept_positions):\n",
    "\n",
    "    common_words = set(normalize_document).intersection(concept_positions)\n",
    "\n",
    "    rank_concept = {word: i + 1 for i, word in enumerate(concept_positions)}\n",
    "    rank_doc = {word: i + 1 for i, word in enumerate(normalize_document)}\n",
    "\n",
    "    # print(rank_concept)\n",
    "    # print(rank_doc)\n",
    "    # print(\"Mots communs:\", common_words)\n",
    "\n",
    "    T = len(common_words)\n",
    "    if T == 0 or T == 1:\n",
    "        return 0  # Pas de mots communs\n",
    "\n",
    "    # Générer les rangs pour les mots communs\n",
    "    doc_ranks = []\n",
    "    concept_ranks = []\n",
    "    for i, word in enumerate(common_words):\n",
    "        doc_ranks.append(rank_doc[word])\n",
    "        concept_ranks.append(rank_concept[word])\n",
    "\n",
    "    # print(\"Rangs du document:\", doc_ranks)\n",
    "    # print(\"Rangs du concept:\", concept_ranks)\n",
    "\n",
    "    return scipy.stats.spearmanr(doc_ranks, concept_ranks).correlation\n",
    "\n",
    "\n",
    "def rel(sim, corr):\n",
    "    return (1 + sim) * (1 + corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cord19/trec-covid documents:   0%|          | 0/192509 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docno</th>\n",
       "      <th>terms_mesh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68064876</td>\n",
       "      <td>safety net provid provid  safety net provid  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82003470</td>\n",
       "      <td>cultur media  pharmacolog action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68018805</td>\n",
       "      <td>sepsi bloodstream infect bloodstream infect in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68045805</td>\n",
       "      <td>mycoplasma synovia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68061387</td>\n",
       "      <td>chlamydi pneumonia chlamydi pneumonia pneumoni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68053490</td>\n",
       "      <td>mink enter viru mink enter virus enter virus  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>68059705</td>\n",
       "      <td>polar bodi bodi  polar bodi  polar polar bodi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>68065207</td>\n",
       "      <td>middl east respiratori syndrom coronaviru mer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68012529</td>\n",
       "      <td>saudi arabia kingdom saudi arabia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      docno                                         terms_mesh\n",
       "0  68064876  safety net provid provid  safety net provid  s...\n",
       "1  82003470                   cultur media  pharmacolog action\n",
       "2  68018805  sepsi bloodstream infect bloodstream infect in...\n",
       "3  68045805                                 mycoplasma synovia\n",
       "4  68061387  chlamydi pneumonia chlamydi pneumonia pneumoni...\n",
       "5  68053490  mink enter viru mink enter virus enter virus  ...\n",
       "6  68059705  polar bodi bodi  polar bodi  polar polar bodi ...\n",
       "7  68065207  middl east respiratori syndrom coronaviru mer ...\n",
       "8  68012529                  saudi arabia kingdom saudi arabia"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_iter = dataset.get_corpus_iter()\n",
    "\n",
    "# Initialize a list to hold the abstracts\n",
    "N_rank = 5\n",
    "\n",
    "# Iterate over the documents\n",
    "for i, doc in enumerate(doc_iter):\n",
    "\n",
    "    df_concepts = pd.DataFrame(\n",
    "        columns=[\"docno\", \"terms_mesh\"]\n",
    "    )  # dataframe des concepts\n",
    "    dic_concepts = {}  # dictionnaire des concepts preferés\n",
    "\n",
    "    # Get the abstract of the document\n",
    "    title = doc.get(\"title\", None)\n",
    "    # abstract = doc.get(\"abstract\", None)\n",
    "    abstract = \"\"\n",
    "\n",
    "    # document ID\n",
    "    docno = doc.get(\"docno\", None)\n",
    "    if docno is None:\n",
    "        print(\"erreur sur le docno\")\n",
    "        break\n",
    "\n",
    "    # If the document has an abstract, add it to the list\n",
    "    if abstract or title:\n",
    "        text = preprocess_text(title + \" \" + abstract, stem=True).split(\" \")\n",
    "\n",
    "    # for each term in the text we will search for the mesh concept\n",
    "    for term in text:\n",
    "        concept_id = mesh_concept(term)\n",
    "        if concept_id == \"None\":\n",
    "            continue\n",
    "        concept, id_term = concept_id\n",
    "\n",
    "        dic_concepts[id_term] = concept  # dictionnaire des concepts preferés\n",
    "        concept_pr = preprocess_text(\" \".join(concept), stem=True).split(\" \")\n",
    "\n",
    "        new_row = pd.DataFrame(\n",
    "            {\"docno\": [id_term], \"terms_mesh\": [\" \".join(concept_pr)]}\n",
    "        )\n",
    "        df_concepts = pd.concat([df_concepts, new_row], ignore_index=True)\n",
    "\n",
    "    # indexer les concepts dans le dataframe\n",
    "    # indexer_tmp = pt.index.DFIndexer(\n",
    "    #     \"C:\\\\Users\\\\Saliou\\\\OneDrive\\\\Documents\\\\ALL TP\\\\RI\\\\projet\\\\tmp\",\n",
    "    #     overwrite=True,\n",
    "    # )\n",
    "\n",
    "    display(df_concepts)\n",
    "    break\n",
    "\n",
    "    index_ref_tmp = indexer_tmp.index(df_concepts[\"terms_mesh\"], df_concepts[\"docno\"])\n",
    "    print(index_ref_tmp)\n",
    "    index_tmp = pt.IndexFactory.of(index_ref_tmp)\n",
    "    print(index_tmp.getCollectionStatistics().toString())\n",
    "\n",
    "    # 1. Calculer la similarité entre le document et chaque concept\n",
    "    # parcourir chaque term du concept et verifier s'il est dans le document\n",
    "    dic_sim = {}\n",
    "    for i, c in enumerate(df_concepts):\n",
    "        entry_terms = c[\"terms_mesh\"]\n",
    "        id_concept = c[\"docno\"]\n",
    "        score = similarite(bm25, index_tmp, docno, id_term, entry_terms)\n",
    "        dic_sim[id_concept] = score\n",
    "\n",
    "    # trier les documents par ordre décroissant\n",
    "    dic_sim = dict(sorted(dic_sim.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # 2. Calculer la corrélation entre le document et le concept\n",
    "    dic_rel = {}\n",
    "\n",
    "    for id_term, sim in dic_sim.items():\n",
    "        Entry_terms = dic_concepts[id_term]\n",
    "\n",
    "        corr_entry = []\n",
    "        for entry in Entry_terms:\n",
    "            Entry_term = preprocess_text(entry, stem=True).split(\" \")\n",
    "\n",
    "            # vecteurs de positions moyennes\n",
    "            vect_entry = word_positions(Entry_term)\n",
    "            vect_doc = word_positions(text)\n",
    "\n",
    "            # fenêtre du document\n",
    "            vect_doc_norm = normalize_document(vect_doc, vect_entry)\n",
    "\n",
    "            # calculer la corrélation\n",
    "            corr_entry.append(spearman_rank_correlation(vect_doc_norm, vect_entry))\n",
    "\n",
    "        # 3. ρ(C, D) = M axE∈Entries(C) ρ(E, D)\n",
    "        corr = max(corr_entry)\n",
    "\n",
    "        # 4. Selecting the candidate concepts for document expansion.\n",
    "        sim = dic_sim[id_term]\n",
    "        rel_score = rel(sim, corr)\n",
    "        dic_rel[id_term] = rel_score\n",
    "\n",
    "    # Expension du document\n",
    "    dic_rel = dict(sorted(dic_rel.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Selecting the top N concepts for document expansion only if the rel score is greater than the threshold\n",
    "    threshold = 2.5\n",
    "    top_concepts = []\n",
    "    for i, (id_term, score) in enumerate(dic_rel.items()):\n",
    "        if i == N_rank:\n",
    "            break\n",
    "        if score >= threshold:\n",
    "            top_concepts.append(id_term)\n",
    "\n",
    "    # 5. Expanding the document with the selected concepts\n",
    "    expanded_text = abstract\n",
    "    if len(top_concepts) == 0:\n",
    "        continue\n",
    "\n",
    "    # Ajouter les termes préférés des concepts sélectionnés\n",
    "    for id_term in top_concepts:\n",
    "        preferred_term = dic_concepts[id_term]\n",
    "        expanded_text = expanded_text + \" \" + preferred_term\n",
    "\n",
    "    # modifier le document original\n",
    "    dataset.get_corpus_iter().update(docno, {\"abstract\": expanded_text})\n",
    "\n",
    "    break\n",
    "\n",
    "# Reindexer le corpus\n",
    "# indexer = pt.index.IterDictIndexer(\n",
    "#     \"C:\\\\Users\\\\Saliou\\\\OneDrive\\\\Documents\\\\ALL TP\\\\RI\\\\projet\\\\cord19-index\",\n",
    "#     overwrite=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'banjoule', 'dog', 'quicke', 'banjoulde', 'lazy', 'the', 'dog6', 'nop', 'mat']\n",
      "['quick', 'banjoule', 'dog', 'quicke', 'banjoulde', 'lazy', 'the', 'dog6', 'nop', 'mat']\n",
      "['mat', 'dog', 'quick']\n",
      "{'mat': 1, 'dog': 2, 'quick': 3}\n",
      "{'quick': 1, 'banjoule': 2, 'dog': 3, 'quicke': 4, 'banjoulde': 5, 'lazy': 6, 'the': 7, 'dog6': 8, 'nop': 9, 'mat': 10}\n",
      "Mots communs: {'dog', 'mat', 'quick'}\n",
      "Rangs du document: [3, 10, 1]\n",
      "Rangs du concept: [2, 1, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_doc = [\n",
    "    \"quick\",\n",
    "    \"banjoule\",\n",
    "    \"dog\",\n",
    "    \"quicke\",\n",
    "    \"banjoulde\",\n",
    "    \"lazy\",\n",
    "    \"the\",\n",
    "    \"dog6\",\n",
    "    \"nop\",\n",
    "    \"mat\",\n",
    "]\n",
    "vect_concept = [\"mat\", \"dog\", \"quick\"]\n",
    "\n",
    "vec = word_positions(vect_doc)\n",
    "print(vec)\n",
    "x = normalize_document(vec, vect_concept)\n",
    "print(x)\n",
    "conc = word_positions(vect_concept)\n",
    "print(conc)\n",
    "spearman_rank_correlation(x, conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseLine BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>P_10</th>\n",
       "      <th>P_20</th>\n",
       "      <th>P_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BR(BM25)</td>\n",
       "      <td>0.207121</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.597333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name       map   P_10  P_20      P_30\n",
       "0  BR(BM25)  0.207121  0.678  0.62  0.597333"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "pt.Experiment(\n",
    "    [bm25],\n",
    "    dataset.get_topics(variant=\"title\"),\n",
    "    dataset.get_qrels(),\n",
    "    eval_metrics=[\"map\", \"P_10\", \"P_20\", \"P_30\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer le modèle BM25\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "# Initialisation de la liste pour stocker les résultats\n",
    "results = []\n",
    "\n",
    "# Définir les paramètres pour l'expansion de requête\n",
    "for num_terms in range(100, 101, 5):\n",
    "    for num_docs in [100]:\n",
    "\n",
    "        qe = pt.rewrite.Bo1QueryExpansion(index, fb_terms=num_terms, fb_docs=num_docs)\n",
    "        pipeline = bm25 >> qe >> bm25\n",
    "        res = pt.Experiment(\n",
    "            [pipeline],\n",
    "            dataset.get_topics(variant=\"title\"),\n",
    "            dataset.get_qrels(),\n",
    "            eval_metrics=[\"map\"],\n",
    "        )\n",
    "        # Ajouter les résultats à la liste\n",
    "        results.append(\n",
    "            {\n",
    "                \"num_terms\": num_terms,\n",
    "                \"num_docs\": num_docs,\n",
    "                \"map\": res[\"map\"][0],\n",
    "            }\n",
    "        )\n",
    "# Convertir les résultats en DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Pivoter le DataFrame pour que num_terms soit les colonnes et num_docs les lignes\n",
    "df_pivot = df_results.pivot(index=\"num_docs\", columns=\"num_terms\", values=\"map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>P_10</th>\n",
       "      <th>P_20</th>\n",
       "      <th>P_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25+Bo1QE</td>\n",
       "      <td>0.233246</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name       map   P_10   P_20   P_30\n",
       "0  BM25+Bo1QE  0.233246  0.658  0.643  0.622"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qe = pt.rewrite.Bo1QueryExpansion(index, fb_terms=20, fb_docs=20)\n",
    "pipeline = bm25 >> qe >> bm25\n",
    "res = pt.Experiment(\n",
    "    [pipeline],\n",
    "    dataset.get_topics(variant=\"title\"),\n",
    "    dataset.get_qrels(),\n",
    "    names=[f\"BM25+Bo1QE\"],\n",
    "    eval_metrics=[\"map\", \"P_10\", \"P_20\", \"P_30\"],\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEcombination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindexer le corpus\n",
    "# indexer = pt.index.IterDictIndexer(\n",
    "#     \"C:\\\\Users\\\\Saliou\\\\OneDrive\\\\Documents\\\\ALL TP\\\\RI\\\\projet\\\\cord19-index\",\n",
    "#     overwrite=True,\n",
    "# )\n",
    "index_aug = \"a definir \"\n",
    "\n",
    "bm25 = pt.BatchRetrieve(index_aug, wmodel=\"BM25\")\n",
    "pt.Experiment(\n",
    "    [bm25],\n",
    "    dataset.get_topics(variant=\"title\"),\n",
    "    dataset.get_qrels(),\n",
    "    eval_metrics=[\"map\", \"P_10\", \"P_20\", \"P_30\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QE+DEcombination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index_aug, wmodel=\"BM25\")\n",
    "qe = pt.rewrite.Bo1QueryExpansion(index, fb_terms=20, fb_docs=20)\n",
    "pipeline = bm25 >> qe >> bm25\n",
    "res = pt.Experiment(\n",
    "    [pipeline],\n",
    "    dataset.get_topics(variant=\"title\"),\n",
    "    dataset.get_qrels(),\n",
    "    names=[f\"BM25+Bo1QE\"],\n",
    "    eval_metrics=[\"map\", \"P_10\", \"P_20\", \"P_30\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
